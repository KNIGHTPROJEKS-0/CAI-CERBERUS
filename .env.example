# CAI-CERBERUS Environment Configuration

# =============================================================================
# MODEL PROVIDER CONFIGURATION
# =============================================================================

# LiteLLM
LITELLM_API_BASE=https://api.litellm.ai
LITELLM_API_KEY=sk-your-litellm-key-here
LITELLM_ORG_ID=org-your-org-id # Optional

# OpenAI
OPENAI_API_KEY=sk-your-openai-key-here
OPENAI_ORG_ID=org-your-org-id # Optional

# Anthropic
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here

# DeepSeek
DEEPSEEK_API_KEY=sk-your-deepseek-key-here

# OpenRouter
OPENROUTER_API_KEY=sk-or-v1-openrouter-key-here

# Google Gemini
GEMINI_API_KEY=AIza-your-gemini-key-here

# Ollama (local models)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=qwen2.5:14b

# =============================================================================
# CERBERUS CORE CONFIGURATION
# =============================================================================

# Default model selection
CERBERUS_MODEL=openai/gpt-4o

# Debug and logging
CERBERUS_DEBUG=1        # 0=minimal, 1=info, 2=verbose
CERBERUS_LOG_LEVEL=INFO # DEBUG, INFO, WARNING, ERROR

# Tracing and observability
CERBERUS_TRACING=true    # Enable OpenTelemetry traces
CERBERUS_TRACE_ENDPOINT= # Optional: custom trace collector

# Safety and limits
CERBERUS_PRICE_LIMIT=10.00 # USD limit per session
CERBERUS_MAX_ITERATIONS=50 # Prevent infinite loops
CERBERUS_TIMEOUT=300       # Seconds per operation

# Workspace and directory configuration
CERBERUS_WORKSPACE=default                   # Default workspace identifier
CERBERUS_WORKSPACE_DIR=./workspaces          # Base directory for workspaces
CERBERUS_EXTERNAL_TOOLS_DIR=./external-tools # External tools directory
CERBERUS_AUDIT_DIR=./audit                   # Audit log directory

# Human-in-the-Loop settings
CERBERUS_HITL_MODE=interactive # interactive, batch, disabled
CERBERUS_AUTO_APPROVE=false    # Require human approval for sensitive ops

# =============================================================================
# SECURITY AND COMPLIANCE
# =============================================================================

# Execution constraints
CERBERUS_SANDBOX_MODE=true              # Enable sandboxed execution
CERBERUS_ALLOWED_HOSTS=                 # Comma-separated list of allowed targets
CERBERUS_BLOCKED_COMMANDS=rm,del,format # Comma-separated blocked commands

# Audit and compliance
CERBERUS_AUDIT_LOG=true   # Enable audit logging
CERBERUS_COMPLIANCE_MODE= # SOC2, HIPAA, etc.

# Rate limiting
CERBERUS_RATE_LIMIT_REQUESTS=60  # Requests per minute
CERBERUS_RATE_LIMIT_CONCURRENT=3 # Concurrent operations

# =============================================================================
# AGENT CONFIGURATION
# =============================================================================

# Default agent settings
CERBERUS_DEFAULT_AGENT=reconnaissance_agent
CERBERUS_AGENT_CONFIG_DIR=./configs/agents
CERBERUS_TOOL_CONFIG_DIR=./configs/tools

# Memory and state
CERBERUS_MEMORY_DIR=./memory
CERBERUS_ENABLE_EPISODIC_MEMORY=true
CERBERUS_ENABLE_SEMANTIC_MEMORY=true
CERBERUS_SESSION_TIMEOUT=3600 # Session timeout in seconds

# =============================================================================
# TOOL CONFIGURATION
# =============================================================================

# Tool discovery and loading
CERBERUS_TOOL_DISCOVERY=auto    # auto, manual, disabled
CERBERUS_TOOL_VALIDATION=strict # strict, permissive, disabled

# External tool paths (auto-detected if not set)
NMAP_PATH=
NUCLEI_PATH=
SUBFINDER_PATH=
AMASS_PATH=

# =============================================================================
# DEVELOPMENT AND TESTING
# =============================================================================

# Development mode
CERBERUS_DEV_MODE=false  # Enable development features
CERBERUS_TEST_MODE=false # Enable test mode with mock tools

# Performance monitoring
CERBERUS_PERFORMANCE_MONITORING=true
CERBERUS_METRICS_ENDPOINT=

# =============================================================================
# INTEGRATION SETTINGS
# =============================================================================

# LiteLLM Proxy settings
LITELLM_MASTER_KEY=sk-cerberus-your-master-key-here
LITELLM_SALT_KEY=your-salt-key-here
LITELLM_PROXY_URL=http://localhost:4000
CERBERUS_LITELLM_ENABLED=true

# MCP (Model Context Protocol) settings
CERBERUS_MCP_ENABLED=true
CERBERUS_MCP_SERVERS_DIR=./external-tools/mcp
CERBERUS_MCP_GATEWAY_URL=http://localhost:3000

# SuperGateway settings
CERBERUS_SUPERGATEWAY_ENABLED=true
CERBERUS_SUPERGATEWAY_PORT=3000

# Metabigor OSINT settings
CERBERUS_OSINT_TOOLS=metabigor
METABIGOR_PATH=./external-tools/reconnaissance/metabigor/metabigor

# Docker Buildx Cloud Builder settings
DOCKER_BUILDKIT=1
COMPOSE_DOCKER_CLI_BUILD=1
BUILDX_BUILDER=cloud-knightprojekslancerr-knight-lancerr-cloudbuilder
DOCKER_CLOUD_BUILDER_ENABLED=true

# Voice interface (if enabled)
CERBERUS_VOICE_ENABLED=false
CERBERUS_VOICE_MODEL=openai/whisper-1
CERBERUS_TTS_MODEL=openai/tts-1

# Web interface (if enabled)
CERBERUS_WEB_ENABLED=false
CERBERUS_WEB_PORT=8080
CERBERUS_WEB_HOST=localhost

# =============================================================================
# RAILWAY DEPLOYMENT
# =============================================================================

RAILWAY_TOKEN=your-railway-token-here
RAILWAY_PROJECT_ID=your-railway-project-id-here
N8N_WEBHOOK_URL=https://your-railway-app.up.railway.app/webhook/cerberus

# =============================================================================
# N8N WORKFLOW AUTOMATION
# =============================================================================

N8N_USER=admin
N8N_PASSWORD=your-secure-password-here
N8N_BASIC_AUTH_ACTIVE=true
N8N_SECURE_COOKIE=false
GENERIC_TIMEZONE=UTC

# =============================================================================
# HUGGING FACE / WHITERABBITNEO CONFIGURATION
# =============================================================================

# Hugging Face Model Configuration
HUGGINGFACE_API_TOKEN=hf_your-huggingface-token-here
WHITERABBITNEO_MODEL=WhiteRabbitNeo/WhiteRabbitNeo-13B-v1
WHITERABBITNEO_API_BASE=http://localhost:8080

# WhiteRabbitNeo Agent Settings
WHITERABBITNEO_MAX_TOKENS=4096
WHITERABBITNEO_TEMPERATURE=0.7
WHITERABBITNEO_TIMEOUT=300

# Resource Limits for Heavy Model
WHITERABBITNEO_GPU_MEMORY=12GB
WHITERABBITNEO_MAX_CONCURRENT=2
WHITERABBITNEO_BATCH_SIZE=1

# LiteLLM HF Configuration
LITELLM_HF_CONFIG_PATH=./configs/litellm-hf.yaml

# =============================================================================
# CODE FUNCTIONS DATASETS
# =============================================================================

CODE_FUNCTIONS_CYBER_PATH=./external-tools/datasets/Code-Functions-Level-Cyber/code-end-to-end-cyber.jsonl
CODE_FUNCTIONS_GENERAL_PATH=./external-tools/datasets/Code-Functions-Level-General/code-end-to-end-general.jsonl
CODE_FUNCTIONS_ENABLED=true

# LLAMACPP (local OpenAI-compatible server via llama-cpp-python)
# Path to your local GGUF model (e.g., ./external-tools/llama-cpp-python/models/codellama-7b.Q4_0.gguf)
LLAMACPP_MODEL=
# Host/port for the local server
LLAMACPP_HOST=0.0.0.0
LLAMACPP_PORT=8080
# Context size and threads (-1 = auto)
LLAMACPP_CTX=4096
LLAMACPP_THREADS=-1
# Metal GPU layers (set 0 to disable GPU offload)
LLAMACPP_N_GPU_LAYERS=1
# Optional chat template format, e.g., llama-2, chatml, mistral-instruct
LLAMACPP_CHAT_FORMAT=
# When using LiteLLM/OpenAI-compatible clients, set base to this server
# Example: http://localhost:8080/v1 (llama-cpp-python exposes /v1)
LLAMACPP_API_BASE=http://localhost:8080/v1
